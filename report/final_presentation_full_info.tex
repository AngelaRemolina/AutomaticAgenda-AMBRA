\documentclass[10pt]{beamer}

% \usetheme{polimi}

% Usage instructions can be found at: https://github.com/elauksap/beamerthemepolimi

% For more documentation, visit:
% https://latex-beamer.com/

\mode<presentation>
{
  \usetheme{polimi}      
  \usecolortheme{default} 
  \usefonttheme{serif}   
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[
backend=biber,
style=authoryear,
sorting=ynt
]{biblatex}
% \usepackage{enumitem}
\addbibresource{bib.bib} 

\title{A9 - Bayesian Additive Regression \\ Trees in spatial data analysis}
\author{Chiappini, Destro, Fraschini, Hagen, Numeroli, \\ Sinning}
\date{February 15, 2024}

\begin{document}

\begin{frame}
    \maketitle
\end{frame}

\begin{frame}{Table of content}
    \tableofcontents
\end{frame}

\AtBeginSection[]
{
    \begin{frame}{Next section}
        \tableofcontents[currentsection]
    \end{frame}
}

\section{Goal}

\begin{frame}{Goal}

    Apply the model in Kim (2022), using their R code for MCMC, to analyse and predict air pollution (specifically PM 2.5 concentration) in Lombardy (using the AGRIMONIA data).

    \vspace{0.2cm}
    \textbf{Specific objectives}
    \begin{itemize}
        \item Understand and rewrite Kim's code to make it more suitable for our purposes. (hard)
        \item Develop an SBART model for predicting PM 2.5 concentration.
        \item Integrate data from other sources to make areal predictions.
        \item Make a simple SIAM matrix.
        \item Incorporate data in SIAM matrix.
    \end{itemize}
\end{frame}

\section{Model}
% Dunno about this one
% ----

\begin{frame}{BART (\cite{Chipman_2010})}
    BART is a Bayesian nonparametric model that employs regression trees and regularization priors to estimate an unknown function, denoted as $f(\underline{x}_i)$, where $\underline{x}_i = (x_{i1}, \dots, x_{ip})$ represents the covariates associated with a response variable $y_i$ in a set of observations $\underline{y} = (y_1, \dots, y_n)$ (with $n$ being the number of observations).

    \begin{block}{Sum of Trees model}
        \begin{align}
            & y_i = f(\underline{x}_i) + \varepsilon_i \approx \sum_{t=1}^m g_t(\underline{x}_i) + \varepsilon_i & \quad \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1, \dots, n \; ,
        \end{align}
    \end{block}

    where $g_t(\underline{x}_i) = g(\underline{x}_i; T_t, M_t)$ represents a tree model, given a tree structure $T_t$ and a set of leaf parameters $M_t$.  
\end{frame}

\begin{frame}{BART (\cite{Chipman_2010})}
    \begin{block}{Likelihood}
        \begin{align}
            y_i \mid \underline{x}_i, \mathcal{T}, \mathcal{M}, \sigma^2 &\stackrel{iid}{\sim} \mathcal{N}\left(\sum_{t=1}^mg\left(\underline{x}_i;  T_t, M_t\right), \sigma^2\right)  
        \end{align}
    \end{block}

    Where,
    \begin{itemize}
        \item $i = 1,...,n$; where $n$ is the number of observations.
        \item $T_t \in \mathcal{T}$; where $\mathcal{T}$ is the set of tree structures.
        \item $M_t \in \mathcal{M}$; where $\mathcal{M}$ is a set of $M_t$'s, $M_t = \{\mu_{t1}, \dots , \mu_{tb_t}\}$ ($\mu_{tj}$, leaf parameters).
        \item $m = |\mathcal{T}| = |\mathcal{M}|$.
    \end{itemize}
\end{frame}

\begin{frame}{BART (\cite{Chipman_2010})}
    \begin{block}{Priors}
        \begin{align}
            \mu_{tj} \mid T_t &\stackrel{iid}{\sim} \mathcal{N}\left(\mu_\mu, \sigma_\mu^2\right) \\
            \sigma^2 &\sim invGamma\left(\frac{\nu}{2}, \frac{\nu\lambda}{2}\right) 
        \end{align}
    \end{block}

    Where hyperparameters are a priori fixed, and
    \begin{itemize}
        \item $t = 1,...,m$
        \item $b_t = |M_t|$
        \item $j = 1,...,b_t$
    \end{itemize}
\end{frame}

\begin{frame}{BART (\cite{Chipman_2010})}
    Given a non-terminal node $N$, let's define:
    \begin{itemize}
        \item $depth(N)$: Depth of node $N$.
        \item $split\_var(N) = x_j$: covariate associated with $N$, $j \in \{1,\dots, p\}$.
        \item $split\_const(N) = c_{ij}$: constant associated with $N$, $i \in \{1,\dots, n\}$, $c_{ij} \in \{x_{1j}, x_{2j}, \dots, x_{nj}\}$.
    \end{itemize}
    Rules for nodes have the form: $split\_var(N) < split\_const(N)$. \bigskip
    
    \textbf{i.e.} Given node $\tilde{N}$, the associated rule is: $x_2 < 10$, where $x_2 = split\_var(\tilde{N})$ and $10 = split\_const(\tilde{N}) \in \{x_{12}, \dots, x_{n2}\}$.
\end{frame}

\begin{frame}{BART (\cite{Chipman_2010})}
   \begin{block}{$T_t$ priors}
        Each $T_t$ is created through a stochastic process that takes into account the following priors for all non-terminal nodes $N$: 
        \begin{enumerate}
            \item $\mathbb{P}\left(depth(N) = d\right) = \frac{\alpha}{\left(1+d\right)^\beta}$.
            \item $\mathbb{P}\left(split\_var(N) = x_j\right) = \frac{1}{p}\mathbb{I}_{\{1,...,p\}}(j)$.
            \item $\mathbb{P}\left(split\_const(N) = c_{ij} \mid x_j\right) = \frac{1}{n}\mathbb{I}_{\{1,...,n\}}(i)\mathbb{I}_{\{1,...,p\}}(j)$. 
        \end{enumerate}
    \end{block}
    \begin{itemize}
    \item $\alpha \in (0,1)$ and $\beta \in [0, \infty)$ are fixed.
        \item $p$ is the number of covariates.
    \end{itemize}
\end{frame}

\begin{frame}{BART (\cite{Chipman_2010})}
   \begin{block}{$T_t$ priors (Alternative view)}
        We can view the selection of splitting rules by introducing new parameters $\underline{\rho}$ and $\underline{s}$ that correspond to selection probabilities for a splitting variable and splitting constant in a node. (\textit{Note: they are the same for all trees, therefore a priori independent from the $T_t$'s})
        \begin{align}
            \rho_j &\stackrel{iid}{\sim} \mathcal{U}(\{1,\dots,p\}) \quad j = 1,\dots,p\\
            s_i &\stackrel{iid}{\sim} \mathcal{U}(\{x_{1j},\dots,x_{nj}\}) \quad i = 1,\dots,n
        \end{align}
    \end{block}
\end{frame}

\begin{frame}{BART (\cite{Chipman_2010})}
   \begin{block}{Fixed values}
        The fixed values in the BART model are the following:
            \begin{itemize}
                \item $m$: Number of trees.
                \item $\nu, \lambda$: Hyperparameters for data-informed $\sigma^2$ prior.
                \item $\alpha, \beta$: Hyperparameters for regularization prior over tree depth.
                \item $\mu_\mu, \sigma_\mu^2$: Hyperparameters for regularization prior over leaf parameters.
                \item $\rho_{CHANGE}$: Probability to induce a change perturbation in tree structure.
                \item $\rho_{GROW}$: Probability to induce a grow perturbation in tree structure.
                \item $\rho_{PRUNE}$: Probability to induce a prune perturbation in tree structure.
            \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{BART (\cite{Chipman_2010})}
    \begin{block}{Priors effects}
        The priors described in previous slides induce the following main effects:
        \begin{itemize}
            \item \textbf{Shrinkage effect}: the prior over $\mu_{tj} \mid T_t$ reduces the individual effects of trees by assigning more mass to $\mu_{tj}$ around $0$. 
            \item \textbf{"Shallowing" effect}: the prior over the depth of tree nodes enforces shallow structures and discourages big trees. 
        \end{itemize}
    \end{block}
    The two effects induced by the priors are at the core of how BART prevents overfitting.
\end{frame}

% \begin{frame}{BART (\cite{Chipman_2010}}
% \begin{block}{Prior shape}
% \begin{equation} \label{priors_restriction}
% \begin{split}
% \pi((T_1, M_1), ... , (T_m, M_m), \sigma) & = \left[\prod_{t = 1}^m \pi(M_t , T_t)\right] \pi(\sigma) \\
% & = \left[\prod_{t = 1}^m \pi(M_t | T_t)\pi(T_t)\right] \pi(\sigma) \\
% & = \left[\prod_{t = 1}^m\pi(T_t) \prod_{j=1}^{b_t} \pi(\mu_{tj} | T_t)\right] \pi(\sigma),
% \end{split}
% \end{equation}
% \end{block}
% \end{frame}

% \begin{frame}{BART (\cite{Chipman_2010})}
%     \begin{block}{Posterior}
%         \begin{equation}
%             \pi((T_1, M_1),...,(T_m, M_m), \sigma \mid \underline{y})
%         \end{equation}
%     \end{block}
%     \textbf{Definition}
%     \begin{itemize}
%         \item $T_{(j)}$: Set of all trees except $T_j$.
%         \item $M_{(j)}$: Set of all tree parameters except $M_j$.
%     \end{itemize}
%     \begin{block}{Metropolis-within-Gibbs}
%         Full conditionals:
%         \begin{itemize}
%             \item $\sigma \mid T_1,...,T_m,M_1,...,M_m, \underline{y}$
%             \item $(T_j, M_j) \mid T_{(j)}, M_{(j)}, \sigma, \underline{y}$
%         \end{itemize}
%     \end{block}
% \end{frame}

\begin{frame}{BART (\cite{Chipman_2010})}
    \begin{block}{Inference from posterior}
        \begin{equation}
            f^*(\cdot) = \sum_{j=1}^mg(\cdot;T^*_j,M^*_j)
        \end{equation}
    \end{block}
    \begin{block}{Posterior mean}
        \begin{equation}
            E(f(\underline{x}_i)|\underline{y}) \approx \frac{1}{K}\sum_{k=1}^Kf^*_k(\underline{x}_i)  \qquad i = 1, \dots, n
        \end{equation}
    \end{block}
    where $K$ is the number of available MCMC draws (excluding a burn-in period and thinning).
\end{frame}

% \begin{frame}{BART (\cite{Chipman_2010})}
%     \begin{block}{Posterior}
%         \begin{equation}
%             \pi((T_1, M_1),...,(T_m, M_m), \sigma \mid \underline{y})
%         \end{equation}
%     \end{block}
%     \textbf{Definition}
%     \begin{itemize}
%         \item $T_{(j)}$: Set of all trees except $T_j$.
%         \item $M_{(j)}$: Set of all tree parameters except $M_j$.
%     \end{itemize}
%     \begin{block}{Metropolis-within-Gibbs}
%         Full conditionals:
%         \begin{itemize}
%             \item $\sigma \mid T_1,...,T_m,M_1,...,M_m, \underline{y}$
%             \item $(T_j, M_j) \mid T_{(j)}, M_{(j)}, \sigma, \underline{y}$
%         \end{itemize}
%     \end{block}
% \end{frame}

% \begin{frame}{BART (\cite{Chipman_2010})}
%     \begin{block}{Inference from posterior}
%         \begin{equation}
%             f^*(\cdot) = \sum_{j=1}^mg(\cdot;T^*_j,M^*_j)
%         \end{equation}
%     \end{block}
%     \begin{block}{Posterior mean}
%         \begin{equation}
%             E(f(\underline{x}_i)|\underline{y}) \approx \frac{1}{K}\sum_{k=1}^Kf^*_k(\underline{x}_i)  \qquad i = 1, \dots, n
%         \end{equation}
%     \end{block}
%     where $K$ is the number of available MCMC draws (excluding a burn-in period and thinning).
% \end{frame}

\subsection{SBART (\cite{Muller_2007})}

\begin{frame}{SBART (\cite{Muller_2007})}
    \textbf{Definition}
    \begin{itemize}
        \item Let $ \theta = \{ \theta_1,...,\theta_n \} $ be the spatial random effect.
    \end{itemize}

    \begin{block}{Likelihood}
        \begin{equation}
            y_i \mid \underline{x}_i, \mathcal{T}, \mathcal{M}, \theta_i, \sigma^2 \stackrel{iid}{\sim} \mathcal{N}\left(\sum_{t=1}^Tg(\underline{x}_i; T_t, M_t) + \theta_i, \sigma^2 \right) \quad i = 1,...,n
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}{SBART (\cite{Muller_2007})}
    \textbf{Definitions}
    \begin{itemize}
        \item Let $w_{ik} \in \{0,1\}$ denote the $(i, k)$th element of a non-negative symmetric n Ã— n adjacency weight matrix W representing the spatial closeness between two locations.
        \item $\rho$ governs spatial smoothing, where high values (approaching 1) indicate strong dependence on neighbors, and low values (approaching 0) imply independence from neighbors in the conditional mean of $\theta_i$.
    \end{itemize}
\end{frame}

\begin{frame}{SBART (\cite{Muller_2007})}
    \textbf{Definition}
    \begin{itemize}
        \item $\theta_{(-i)}$ denotes all the elements of $\theta$ except $\theta_i$.
    \end{itemize}

    \begin{block}{Priors}
        \begin{align}
            \theta_i \mid \theta_{(-i)}, \rho, \tau^2 &\stackrel{ind}{\sim} \mathcal{N}\left(\frac{\rho}{\sum^n_{k=1}w_{ik}}\sum_{k\neq i}w_{ik}\theta_k,\frac{\tau^2}{\sum_{k=1}^nw_{ik}}\right)\\
            \tau^2 &\sim invGamma(a, b)\\
            \rho &\sim \mathcal{U}(0,1)
        \end{align}
    \end{block}
    Where $a$ and $b$ are fixed hyperparameters.
\end{frame}

\subsection{SBART (\cite{Kim_2022})}

\begin{frame}{SBART (\cite{Kim_2022})}
    \textbf{Definitions}
    \begin{itemize}
        \item $i \leftrightarrow j$ denotes that the ith and jth locations are spatially connected.
        \item $\mathcal{F}_d$ denotes the set of candidates for the distance function.
        \item $f(d_{ij})$ denotes a function of the distance between locations $i$ and $j$.
    \end{itemize}
    
    \begin{block}{SIAM (Structurally informed adjacency matrix)}
        \begin{equation}
            \textbf{W}(d) = (w_{ij}), \quad w_{ij} = \frac{1}{f(d_{ij})}I(i \leftrightarrow j) \in [0,1], \quad f \in \mathcal{F}_d
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}{SIAM}
    There are many ways to build the SIAM matrix. We ended up using the one Kim used in his example in his paper.
    We consider two municipalities adjacent if their centroids are within a certain angle parameter to the left and right of the prevailing wind direction. Following is the algorithm we developed to calculate the wind adjacency matrix.
    \begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{images/vectors.png}
    \caption{Diagram of the vectors used in the algorithm}
    \label{fig:vectors}
\end{figure}
\end{frame}

\begin{frame}{SBART (\cite{Kim_2022})}
    \begin{block}{Prior}
        \begin{equation}
            \theta_i \mid \theta_{(-i)}, \rho, \tau^2 \stackrel{ind}{\sim} \mathcal{N}\left( \frac{\rho\sum_{k=1}^nw_{ik}\theta_k}{\rho\sum_{k=1}^nw_{ik}+1-\rho},\frac{\tau^2}{\rho\sum_{k=1}^nw_{ik}+1-\rho}\right)
        \end{equation}
    \end{block}
    
    \begin{block}{Sparsity-inducing prior}
        \begin{equation}
            (s_1, ... ,s_P) \sim Dirichlet\left(\frac{\alpha}{P}, ... ,\frac{\alpha}{P} \right)
        \end{equation}
    \end{block}
    Where 
    \begin{itemize}
        \item $P$ is the number of covariates.
        \item $s_i$ is the probability of choosing a covariate $i \in {1,...,P}$.
    \end{itemize}
    
    
\end{frame}

\begin{frame}{SBART (\cite{Kim_2022})}
    \begin{block}{Posterior}
        \begin{equation}
            \pi(T_1,...,T_t, M_1,...,M_t, \sigma^2, \underline{\theta}, \tau^2, \rho, \alpha, f \mid \underline{y})
        \end{equation}
    \end{block}
    For drawing from the posterior, it uses a Metropolis-within-Gibbs sampler (\cite{Kim_2022}, Algorithm 1).
\end{frame}

\section{Dataset}


\begin{frame}{AgrImOnIA}
    The dataset contains daily values of air quality, weather, emissions, livestock, and land and soil use in the Lombardy region, Italy, from 2016 to 2021.
    \begin{block}{Categories collected}
        \begin{itemize}
            \item Air quality.
            \item Weather data.
            \item Land allocation.
            \item Emissions.
            \item Livestock.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{AgrImOnIA}
    \begin{figure}
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\linewidth]{images/output_stations.png}
            \caption{Monitoring stations in AgrImOnIA Dataset.}
        \end{subfigure}%
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\linewidth]{images/output_stations_mun.png}
            \caption{Monitoring stations in AgrImOnIA Dataset against municipalities.}
        \end{subfigure}
    \end{figure}
\end{frame}

\begin{frame}{Covariates}
    \begin{figure}
        \includegraphics[width=0.5\textwidth]{images/covariates_grid.png}
        \caption{Grid of points for the covariates of interest.}
    \end{figure}
\end{frame}

\begin{frame}{PM2.5}
    \begin{figure}
        \includegraphics[width=0.6\textwidth]{images/stations_with_pm25.png}
        \caption{Stations with PM2.5 data.}
    \end{figure}
\end{frame}

\begin{frame}{PM2.5}
    \begin{figure}
        \includegraphics[width=\textwidth]{images/pm25_time.png}
        \caption{PM2.5 time series.}
    \end{figure}

    This data corresponds to the PM2.5 concentration in the air taking the average of all the stations within the dataset.
\end{frame}

\begin{frame}{Interpolation}
    Since our covariates of interest in the dataset are given in a grid of points, to be able to predict the PM2.5 concentration in the municipalities we need to interpolate the data.

    \begin{block}{Interpolation method}
        The interpolation was made making a square around each point of covariates, then we intersected this grid with the lombardy municipalities and took for every covariate the weighted average by the area of the intersection.
    \end{block}

\end{frame}

\begin{frame}{Interpolation}
    \begin{figure}
        \includegraphics[width=0.45\textwidth]{images/interpolation_example.png}
        \caption{Interpolation method.}
    \end{figure}
\end{frame}

\begin{frame}{Weight matrices}
    \begin{figure}
        \includegraphics[width=0.7\textwidth]{images/ws.png}
        \caption{Weight matrices.}
    \end{figure}
\end{frame}

\begin{frame}{Wind adjacency}
    \begin{figure}
        \includegraphics[width=0.5\textwidth]{images/wind.png}
        \caption{Wind adjacency.}
    \end{figure}
\end{frame}

\section{First approach}

\begin{frame}{Results}
    \begin{figure}
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=\linewidth]{images/First_approach_2018.png}
            \caption{Predictions made by the model for the PM2.5 concentration in the Lombardy municipalities in 2018.}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=\linewidth]{images/cov_sel.png}
            \caption{Frequency of covariate selection.}
        \end{minipage}
    \end{figure}
\end{frame}

\begin{frame}{Conclusions}
    \begin{itemize}
        \item Promising results.
        \item Too many predictions to make.
        \item Heavy computation (4 hours in average per execution).
              \begin{itemize}
                  \item 2000 iterations (below the recommended 10000).
                  \item 200 trees.
                  \item 200 burn-in.
              \end{itemize}
    \end{itemize}
    \begin{block}{A new approach is needed}
        We need to find a way to make the model more efficient by reducing the number of areas.
    \end{block}
\end{frame}

\section{Second approach}

%\begin{frame}{Data pre-processing}
    %This time we decided to keep the municipalities where there are stations and convert the rest of Lombardy to squares and interpolate the data for the covariates of interest.
%\end{frame}

\begin{frame}{Data pre-processing}
    \begin{figure}
        \includegraphics[width=0.8\textwidth]{images/interpolation_method_2.png}
        \caption{Example of new areas generation}
    \end{figure}
\end{frame}

\begin{frame}{New Areas}
    \begin{figure}
        \includegraphics[width=0.65\textwidth]{images/new_lombardy.jpeg}
        \caption{}
    \end{figure}
\end{frame}

\begin{frame}{Distance cost matrices}
    \begin{figure}
        \includegraphics[width=0.77\textwidth]{images/ws_2.png}
        \caption{}
    \end{figure}
\end{frame}

\begin{frame}{Wind adjacency matrix}
    \begin{figure}
        \includegraphics[width=0.57\textwidth]{images/wind_2.png}
    \end{figure}
\end{frame}

\begin{frame}{Results: Predicted values}
    \begin{figure}
        \includegraphics[width=0.7\linewidth]{images/2018_low_season_model_30wind_10m_0_da_map.png}
        \caption{Predicted values for summer of 2018}
    \end{figure}
\end{frame}

\begin{frame}{Results: covariate selection}
    \begin{figure}
        \includegraphics[width=0.65\linewidth]{images/2018_low_season_model_60wind_10m_0_cov_inclusion_cov_inclusion.png}
        \caption{}
    \end{figure}
\end{frame}

\begin{frame}{Results: leave one out test}
    \begin{figure}
        \includegraphics[width=\linewidth]{images/2018_all_year_milan_missing.png}
        \caption{Predictions made using the wind adjacency matrix}
    \end{figure}
\end{frame}

\begin{frame}{Results: simple adjacency vs SIAM}
    \begin{figure}
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=\linewidth]{images/2019_2020_winter_simple_adjacency.png}
            \caption{Predictions made using just distance and adjacency information}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \includegraphics[width=\linewidth]{images/2019_2020_winter_wind_adjacency.png}
            \caption{Predictions made using the wind adjacency matrix}
        \end{minipage}
    \end{figure}
\end{frame}

\begin{frame}{Results: correlation with farms}
    \begin{figure}
        \includegraphics[width=\linewidth]{images/pigs_and_bovines.png}
        \caption{Number of pigs (left) and bovines (right) from the grid covariate dataset}
    \end{figure}
\end{frame}

\section{Conclusions}
\begin{frame}{What we've learned}
    \begin{itemize}
        \item R is slow and documentation is king.
        \item Kim's new model is capable of making realistic predictions
        \item Expanding the model to use time series instead of just one data point per area might be a good idea for future work.
        \item There is an evident correlation between $PM_{2.5}$ and agriculture density.
    \end{itemize}
\end{frame}

\section{Bibliography}

\begin{frame}{Bibliography}

    \printbibliography

\end{frame}

\section{Acknowledgements}
\begin{frame}{Acknowledgements}
    We would like to thank Alessandro Carminati and Paolo Maranzano for their insightful assistance.
\end{frame}

\end{document}
