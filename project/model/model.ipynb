{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has the model creation for first time running with original data. The retraining on real time with ongoing data is on file `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Angela\\anaconda3\\envs\\ACA\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import json\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_activities = []\n",
    "with open(\"data_activities.json\", \"r\") as f:\n",
    "    data_activities = json.load(f)\n",
    "\n",
    "data_feedback = []\n",
    "with open(\"data_feedback.json\", \"r\") as f:\n",
    "    data_feedback = json.load(f)\n",
    "\n",
    "unique_activity_ids = set(activity[\"act_id\"] for activity in data_activities)\n",
    "unique_user_ids = set(u[\"user_id\"] for u in data_feedback)\n",
    "\n",
    "# convert to bytes (tensorflow requires)\n",
    "unique_user_ids = np.array(list(map(str.encode, unique_user_ids)))\n",
    "unique_activity_ids = np.array(list(map(str.encode, unique_activity_ids)))\n",
    "\n",
    "# cast activity data into TF tensor\n",
    "activities_list = [act[\"act_id\"] for act in data_activities]\n",
    "activities = tf.data.Dataset.from_tensor_slices(activities_list)\n",
    "\n",
    "# cast feedback data into TF tensor\n",
    "user_id_list = []\n",
    "act_id_list = []\n",
    "act_class_list = []\n",
    "for entry in data_feedback:\n",
    "    user_id_list.append(entry[\"user_id\"])\n",
    "    act_id_list.append(entry[\"act_id\"])\n",
    "    # from 'act_id' extract also the activity class\n",
    "    act_class_list.append(\n",
    "        next(filter(lambda x: x[\"act_id\"] == entry[\"act_id\"], data_activities), {}).get(\n",
    "            \"act_class\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "feedback = tf.data.Dataset.from_tensor_slices(\n",
    "    {\n",
    "        \"user_id\": user_id_list,\n",
    "        \"act_id\": act_id_list,\n",
    "        \"act_class\": act_class_list,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 32  # Higher values will correspond to models that may be more accurate, but will also be slower to fit and more prone to overfitting.\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCH = 3\n",
    "NUM_SAMPLES = len(data_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------Train/test split ---------------\n",
    "shuffled = feedback.shuffle(buffer_size=NUM_SAMPLES, reshuffle_each_iteration=False)\n",
    "\n",
    "train_percent = 0.8  # 0.2 test\n",
    "\n",
    "train_size = int(train_percent * NUM_SAMPLES)\n",
    "test_size = NUM_SAMPLES - train_size\n",
    "\n",
    "train = shuffled.take(train_size)\n",
    "test = shuffled.skip(train_size).take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique activities: 11\n",
      "Unique users: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique activities: {}\".format(len(unique_activity_ids)))\n",
    "print(\"Unique users: {}\".format(len(unique_user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, retrieval_weight: float) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.activity_model: tf.keras.layers.Layer = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_activity_ids, mask_token=None\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    len(unique_activity_ids) + 1, EMBEDDING_DIMENSION\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.user_model: tf.keras.layers.Layer = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_user_ids, mask_token=None\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    len(unique_user_ids) + 1, EMBEDDING_DIMENSION\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=activities.batch(128).map(self.activity_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.retrieval_weight = retrieval_weight\n",
    "\n",
    "    def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        # And pick out the activity features and pass them into the activity model.\n",
    "        activity_embeddings = self.activity_model(features[\"act_id\"])\n",
    "\n",
    "        return (user_embeddings, activity_embeddings)\n",
    "\n",
    "    def compute_loss(\n",
    "        self, features: Dict[Text, tf.Tensor], training=False\n",
    "    ) -> tf.Tensor:\n",
    "\n",
    "        user_embeddings, activity_embeddings = self(features)\n",
    "\n",
    "        retrieval_loss = self.retrieval_task(user_embeddings, activity_embeddings)\n",
    "\n",
    "        return self.retrieval_weight * retrieval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Angela\\anaconda3\\envs\\ACA\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Angela\\anaconda3\\envs\\ACA\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 4s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.4286 - factorized_top_k/top_10_categorical_accuracy: 0.7143 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 13.6161 - regularization_loss: 0.0000e+00 - total_loss: 13.6161\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 132ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.7143 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 13.5712 - regularization_loss: 0.0000e+00 - total_loss: 13.5712\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 117ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.7143 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 13.5091 - regularization_loss: 0.0000e+00 - total_loss: 13.5091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x221c02e08d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ActivityModel(retrieval_weight=1.0)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "cached_train = train.shuffle(NUM_SAMPLES).batch(BATCH_SIZE).cache()\n",
    "cached_test = test.batch(BATCH_SIZE).cache()\n",
    "\n",
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 875ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 1.0000 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 1.3493 - regularization_loss: 0.0000e+00 - total_loss: 1.3493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 1.0,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 1.0,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 1.0,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 1.0,\n",
       " 'loss': 1.349341630935669,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 1.349341630935669}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = model.evaluate(cached_test, return_dict=True)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"activity_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 32)                384       \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 32)                128       \n",
      "                                                                 \n",
      " retrieval (Retrieval)       multiple                  1         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513 (2.00 KB)\n",
      "Trainable params: 512 (2.00 KB)\n",
      "Non-trainable params: 1 (4.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(user, top_n=3):\n",
    "\n",
    "    # Create a model that takes in raw query features, and\n",
    "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "    # recommends activities out of the entire activity dataset.\n",
    "    index.index_from_dataset(\n",
    "        tf.data.Dataset.zip(\n",
    "            (\n",
    "                activities.batch(BATCH_SIZE),\n",
    "                activities.batch(BATCH_SIZE).map(model.activity_model),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Get recommendations.\n",
    "    _, titles = index(tf.constant([str(user)]))\n",
    "\n",
    "    print(\"Top {} recommendations for user {}:\\n\".format(top_n, user))\n",
    "    for i, title in enumerate(titles[0, :top_n].numpy()):\n",
    "        print(\"{}. {}\".format(i + 1, title.decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommendations for user U3:\n",
      "\n",
      "1. A4\n",
      "2. A2\n",
      "3. A5\n",
      "4. A3\n",
      "5. A7\n"
     ]
    }
   ],
   "source": [
    "predicting(\"U3\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Angela\\anaconda3\\envs\\ACA\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Angela\\anaconda3\\envs\\ACA\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\save_impl.py:66: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow_recommenders.tasks.retrieval.Retrieval object at 0x00000221C183D310>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: ./trained_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./trained_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/70358350/how-to-deal-with-tf-saved-model-savemodel-filepath-error\n",
    "model.retrieval_task = tfrs.tasks.Retrieval()  # Removes the metrics.\n",
    "model.compile()\n",
    "model.save(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Angela\\anaconda3\\envs\\ACA\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Angela\\anaconda3\\envs\\ACA\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.saving.legacy.saved_model.load.ActivityModel at 0x221c28b6ed0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"./trained_model\")\n",
    "loaded_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
